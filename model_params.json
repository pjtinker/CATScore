{
    "RandomForestClassifier": {
        "doc": "A random forest classifier.\n\n    A random forest is a meta estimator that fits a number of decision tree\n    classifiers on various sub-samples of the dataset and use averaging to\n    improve the predictive accuracy and control over-fitting.\n    The sub-sample size is always the same as the original\n    input sample size but the samples are drawn with replacement if\n    `bootstrap=True` (default).\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : integer, optional (default=10)\n        The number of trees in the forest.\n\n    criterion : string, optional (default=\"gini\")\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n        Note: this parameter is tree-specific.\n\n    max_features : int, float, string or None, optional (default=\"auto\")\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a percentage and\n          `int(max_features * n_features)` features are considered at each\n          split.\n        - If \"auto\", then `max_features=sqrt(n_features)`.\n        - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_depth : integer or None, optional (default=None)\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int, float, optional (default=2)\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a percentage and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for percentages.\n\n    min_samples_leaf : int, float, optional (default=1)\n        The minimum number of samples required to be at a leaf node:\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a percentage and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for percentages.\n\n    min_weight_fraction_leaf : float, optional (default=0.)\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_leaf_nodes : int or None, optional (default=None)\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_split : float,\n        Threshold for early stopping in tree growth. A node will split\n        if its impurity is above the threshold, otherwise it is a leaf.\n\n        .. deprecated:: 0.19\n           ``min_impurity_split`` has been deprecated in favor of\n           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n           Use ``min_impurity_decrease`` instead.\n\n    min_impurity_decrease : float, optional (default=0.)\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    bootstrap : boolean, optional (default=True)\n        Whether bootstrap samples are used when building trees.\n\n    oob_score : bool (default=False)\n        Whether to use out-of-bag samples to estimate\n        the generalization accuracy.\n\n    n_jobs : integer, optional (default=1)\n        The number of jobs to run in parallel for both `fit` and `predict`.\n        If -1, then the number of jobs is set to the number of cores.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    verbose : int, optional (default=0)\n        Controls the verbosity of the tree building process.\n\n    warm_start : bool, optional (default=False)\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest.\n\n    class_weight : dict, list of dicts, \"balanced\",\n        \"balanced_subsample\" or None, optional (default=None)\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        Note that for multioutput (including multilabel) weights should be\n        defined for each class of every column in its own dict. For example,\n        for four-class multilabel classification weights should be\n        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n        [{1:1}, {2:5}, {3:1}, {4:1}].\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n        weights are computed based on the bootstrap sample for every tree\n        grown.\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    Attributes\n    ----------\n    estimators_ : list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    classes_ : array of shape = [n_classes] or a list of such arrays\n        The classes labels (single output problem), or a list of arrays of\n        class labels (multi-output problem).\n\n    n_classes_ : int or list\n        The number of classes (single output problem), or a list containing the\n        number of classes for each output (multi-output problem).\n\n    n_features_ : int\n        The number of features when ``fit`` is performed.\n\n    n_outputs_ : int\n        The number of outputs when ``fit`` is performed.\n\n    feature_importances_ : array of shape = [n_features]\n        The feature importances (the higher, the more important the feature).\n\n    oob_score_ : float\n        Score of the training dataset obtained using an out-of-bag estimate.\n\n    oob_decision_function_ : array of shape = [n_samples, n_classes]\n        Decision function computed with out-of-bag estimate on the training\n        set. If n_estimators is small it might be possible that a data point\n        was never left out during the bootstrap. In this case,\n        `oob_decision_function_` might contain NaN.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.datasets import make_classification\n    >>>\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n    >>> clf.fit(X, y)\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                max_depth=2, max_features='auto', max_leaf_nodes=None,\n                min_impurity_decrease=0.0, min_impurity_split=None,\n                min_samples_leaf=1, min_samples_split=2,\n                min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n                oob_score=False, random_state=0, verbose=0, warm_start=False)\n    >>> print(clf.feature_importances_)\n    [ 0.17287856  0.80608704  0.01884792  0.00218648]\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]\n\n    Notes\n    -----\n    The default values for the parameters controlling the size of the trees\n    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n    unpruned trees which can potentially be very large on some data sets. To\n    reduce memory consumption, the complexity and size of the trees should be\n    controlled by setting those parameter values.\n\n    The features are always randomly permuted at each split. Therefore,\n    the best found split may vary, even with the same training data,\n    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n    of the criterion is identical for several splits enumerated during the\n    search of the best split. To obtain a deterministic behaviour during\n    fitting, ``random_state`` has to be fixed.\n\n    References\n    ----------\n\n    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\n    See also\n    --------\n    DecisionTreeClassifier, ExtraTreesClassifier\n    ",
        "Hyperparameters": {
            "oob_score": false,
            "bootstrap": true,
            "max_features": "auto",
            "class_weight": null,
            "random_state": null,
            "min_samples_split": 2,
            "min_impurity_split": null,
            "verbose": 0,
            "min_weight_fraction_leaf": 0.0,
            "criterion": "gini",
            "warm_start": false,
            "min_impurity_decrease": 0.0,
            "max_leaf_nodes": null,
            "max_depth": null,
            "n_jobs": 1,
            "min_samples_leaf": 1,
            "n_estimators": 10
        }
    },
    "SVC": {
        "doc": "C-Support Vector Classification.\n\n    The implementation is based on libsvm. The fit time complexity\n    is more than quadratic with the number of samples which makes it hard\n    to scale to dataset with more than a couple of 10000 samples.\n\n    The multiclass support is handled according to a one-vs-one scheme.\n\n    For details on the precise mathematical formulation of the provided\n    kernel functions and how `gamma`, `coef0` and `degree` affect each\n    other, see the corresponding section in the narrative documentation:\n    :ref:`svm_kernels`.\n\n    Read more in the :ref:`User Guide <svm_classification>`.\n\n    Parameters\n    ----------\n    C : float, optional (default=1.0)\n        Penalty parameter C of the error term.\n\n    kernel : string, optional (default='rbf')\n         Specifies the kernel type to be used in the algorithm.\n         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n         a callable.\n         If none is given, 'rbf' will be used. If a callable is given it is\n         used to pre-compute the kernel matrix from data matrices; that matrix\n         should be an array of shape ``(n_samples, n_samples)``.\n\n    degree : int, optional (default=3)\n        Degree of the polynomial kernel function ('poly').\n        Ignored by all other kernels.\n\n    gamma : float, optional (default='auto')\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n        If gamma is 'auto' then 1/n_features will be used instead.\n\n    coef0 : float, optional (default=0.0)\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    probability : boolean, optional (default=False)\n        Whether to enable probability estimates. This must be enabled prior\n        to calling `fit`, and will slow down that method.\n\n    shrinking : boolean, optional (default=True)\n        Whether to use the shrinking heuristic.\n\n    tol : float, optional (default=1e-3)\n        Tolerance for stopping criterion.\n\n    cache_size : float, optional\n        Specify the size of the kernel cache (in MB).\n\n    class_weight : {dict, 'balanced'}, optional\n        Set the parameter C of class i to class_weight[i]*C for\n        SVC. If not given, all classes are supposed to have\n        weight one.\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n    verbose : bool, default: False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, optional (default=-1)\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    decision_function_shape : 'ovo', 'ovr', default='ovr'\n        Whether to return a one-vs-rest ('ovr') decision function of shape\n        (n_samples, n_classes) as all other classifiers, or the original\n        one-vs-one ('ovo') decision function of libsvm which has shape\n        (n_samples, n_classes * (n_classes - 1) / 2).\n\n        .. versionchanged:: 0.19\n            decision_function_shape is 'ovr' by default.\n\n        .. versionadded:: 0.17\n           *decision_function_shape='ovr'* is recommended.\n\n        .. versionchanged:: 0.17\n           Deprecated *decision_function_shape='ovo' and None*.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        The seed of the pseudo random number generator to use when shuffling\n        the data.  If int, random_state is the seed used by the random number\n        generator; If RandomState instance, random_state is the random number\n        generator; If None, the random number generator is the RandomState\n        instance used by `np.random`.\n\n    Attributes\n    ----------\n    support_ : array-like, shape = [n_SV]\n        Indices of support vectors.\n\n    support_vectors_ : array-like, shape = [n_SV, n_features]\n        Support vectors.\n\n    n_support_ : array-like, dtype=int32, shape = [n_class]\n        Number of support vectors for each class.\n\n    dual_coef_ : array, shape = [n_class-1, n_SV]\n        Coefficients of the support vector in the decision function.\n        For multiclass, coefficient for all 1-vs-1 classifiers.\n        The layout of the coefficients in the multiclass case is somewhat\n        non-trivial. See the section about multi-class classification in the\n        SVM section of the User Guide for details.\n\n    coef_ : array, shape = [n_class-1, n_features]\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is a readonly property derived from `dual_coef_` and\n        `support_vectors_`.\n\n    intercept_ : array, shape = [n_class * (n_class-1) / 2]\n        Constants in decision function.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> y = np.array([1, 1, 2, 2])\n    >>> from sklearn.svm import SVC\n    >>> clf = SVC()\n    >>> clf.fit(X, y) #doctest: +NORMALIZE_WHITESPACE\n    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n        max_iter=-1, probability=False, random_state=None, shrinking=True,\n        tol=0.001, verbose=False)\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n\n    See also\n    --------\n    SVR\n        Support Vector Machine for Regression implemented using libsvm.\n\n    LinearSVC\n        Scalable Linear Support Vector Machine for classification\n        implemented using liblinear. Check the See also section of\n        LinearSVC for more comparison element.\n\n    ",
        "Hyperparameters": {
            "shrinking": true,
            "max_iter": -1,
            "cache_size": 200,
            "class_weight": null,
            "kernel": "rbf",
            "gamma": "auto",
            "verbose": false,
            "C": 1.0,
            "tol": 0.001,
            "probability": false,
            "coef0": 0.0,
            "degree": 3,
            "random_state": null,
            "decision_function_shape": "ovr"
        }
    },
    "XGBClassifier": {
        "doc": "Implementation of the scikit-learn API for XGBoost classification.\n\n        Parameters\n    ----------\n    max_depth : int\n        Maximum tree depth for base learners.\n    learning_rate : float\n        Boosting learning rate (xgb's \"eta\")\n    n_estimators : int\n        Number of boosted trees to fit.\n    silent : boolean\n        Whether to print messages while running boosting.\n    objective : string or callable\n        Specify the learning task and the corresponding learning objective or\n        a custom objective function to be used (see note below).\n    booster: string\n        Specify which booster to use: gbtree, gblinear or dart.\n    nthread : int\n        Number of parallel threads used to run xgboost.  (Deprecated, please use n_jobs)\n    n_jobs : int\n        Number of parallel threads used to run xgboost.  (replaces nthread)\n    gamma : float\n        Minimum loss reduction required to make a further partition on a leaf node of the tree.\n    min_child_weight : int\n        Minimum sum of instance weight(hessian) needed in a child.\n    max_delta_step : int\n        Maximum delta step we allow each tree's weight estimation to be.\n    subsample : float\n        Subsample ratio of the training instance.\n    colsample_bytree : float\n        Subsample ratio of columns when constructing each tree.\n    colsample_bylevel : float\n        Subsample ratio of columns for each split, in each level.\n    reg_alpha : float (xgb's alpha)\n        L1 regularization term on weights\n    reg_lambda : float (xgb's lambda)\n        L2 regularization term on weights\n    scale_pos_weight : float\n        Balancing of positive and negative weights.\n    base_score:\n        The initial prediction score of all instances, global bias.\n    seed : int\n        Random number seed.  (Deprecated, please use random_state)\n    random_state : int\n        Random number seed.  (replaces seed)\n    missing : float, optional\n        Value in the data which needs to be present as a missing value. If\n        None, defaults to np.nan.\n    **kwargs : dict, optional\n        Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n        be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.md.\n        Attempting to set a parameter via the constructor args and **kwargs dict simultaneously\n        will result in a TypeError.\n        Note:\n            **kwargs is unsupported by Sklearn.  We do not guarantee that parameters passed via\n            this argument will interact properly with Sklearn.\n\n    Note\n    ----\n    A custom objective function can be provided for the ``objective``\n    parameter. In this case, it should have the signature\n    ``objective(y_true, y_pred) -> grad, hess``:\n\n    y_true: array_like of shape [n_samples]\n        The target values\n    y_pred: array_like of shape [n_samples]\n        The predicted values\n\n    grad: array_like of shape [n_samples]\n        The value of the gradient for each sample point.\n    hess: array_like of shape [n_samples]\n        The value of the second derivative for each sample point\n    ",
        "Hyperparameters": {
            "objective": "binary:logistic",
            "max_delta_step": 0,
            "seed": null,
            "base_score": 0.5,
            "missing": null,
            "min_child_weight": 1,
            "colsample_bylevel": 1,
            "learning_rate": 0.1,
            "gamma": 0,
            "reg_alpha": 0,
            "random_state": 0,
            "scale_pos_weight": 1,
            "colsample_bytree": 1,
            "booster": "gbtree",
            "nthread": null,
            "silent": true,
            "subsample": 1,
            "max_depth": 3,
            "n_jobs": 1,
            "reg_lambda": 1,
            "n_estimators": 100
        }
    },
    "MultinomialNB": {
        "doc": "\n    Naive Bayes classifier for multinomial models\n\n    The multinomial Naive Bayes classifier is suitable for classification with\n    discrete features (e.g., word counts for text classification). The\n    multinomial distribution normally requires integer feature counts. However,\n    in practice, fractional counts such as tf-idf may also work.\n\n    Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n\n    Parameters\n    ----------\n    alpha : float, optional (default=1.0)\n        Additive (Laplace/Lidstone) smoothing parameter\n        (0 for no smoothing).\n\n    fit_prior : boolean, optional (default=True)\n        Whether to learn class prior probabilities or not.\n        If false, a uniform prior will be used.\n\n    class_prior : array-like, size (n_classes,), optional (default=None)\n        Prior probabilities of the classes. If specified the priors are not\n        adjusted according to the data.\n\n    Attributes\n    ----------\n    class_log_prior_ : array, shape (n_classes, )\n        Smoothed empirical log probability for each class.\n\n    intercept_ : property\n        Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n        as a linear model.\n\n    feature_log_prob_ : array, shape (n_classes, n_features)\n        Empirical log probability of features\n        given a class, ``P(x_i|y)``.\n\n    coef_ : property\n        Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n        as a linear model.\n\n    class_count_ : array, shape (n_classes,)\n        Number of samples encountered for each class during fitting. This\n        value is weighted by the sample weight when provided.\n\n    feature_count_ : array, shape (n_classes, n_features)\n        Number of samples encountered for each (class, feature)\n        during fitting. This value is weighted by the sample weight when\n        provided.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.random.randint(5, size=(6, 100))\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\n    >>> from sklearn.naive_bayes import MultinomialNB\n    >>> clf = MultinomialNB()\n    >>> clf.fit(X, y)\n    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n    >>> print(clf.predict(X[2:3]))\n    [3]\n\n    Notes\n    -----\n    For the rationale behind the names `coef_` and `intercept_`, i.e.\n    naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n    Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n\n    References\n    ----------\n    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n    Information Retrieval. Cambridge University Press, pp. 234-265.\n    http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n    ",
        "Hyperparameters": {
            "class_prior": null,
            "fit_prior": true,
            "alpha": 1.0
        }
    },
    "TfidfVectorizer": {
        "doc": "Convert a collection of raw documents to a matrix of TF-IDF features.\n\n    Equivalent to CountVectorizer followed by TfidfTransformer.\n\n    Read more in the :ref:`User Guide <text_feature_extraction>`.\n\n    Parameters\n    ----------\n    input : string {'filename', 'file', 'content'}\n        If 'filename', the sequence passed as an argument to fit is\n        expected to be a list of filenames that need reading to fetch\n        the raw content to analyze.\n\n        If 'file', the sequence items must have a 'read' method (file-like\n        object) that is called to fetch the bytes in memory.\n\n        Otherwise the input is expected to be the sequence strings or\n        bytes items are expected to be analyzed directly.\n\n    encoding : string, 'utf-8' by default.\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {'strict', 'ignore', 'replace'}\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        'strict', meaning that a UnicodeDecodeError will be raised. Other\n        values are 'ignore' and 'replace'.\n\n    strip_accents : {'ascii', 'unicode', None}\n        Remove accents during the preprocessing step.\n        'ascii' is a fast method that only works on characters that have\n        an direct ASCII mapping.\n        'unicode' is a slightly slower method that works on any characters.\n        None (default) does nothing.\n\n    analyzer : string, {'word', 'char'} or callable\n        Whether the feature should be made of word or character n-grams.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n    preprocessor : callable or None (default)\n        Override the preprocessing (string transformation) stage while\n        preserving the tokenizing and n-grams generation steps.\n\n    tokenizer : callable or None (default)\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == 'word'``.\n\n    ngram_range : tuple (min_n, max_n)\n        The lower and upper boundary of the range of n-values for different\n        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n        will be used.\n\n    stop_words : string {'english'}, list, or None (default)\n        If a string, it is passed to _check_stop_list and the appropriate stop\n        list is returned. 'english' is currently the only supported string\n        value.\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == 'word'``.\n\n        If None, no stop words will be used. max_df can be set to a value\n        in the range [0.7, 1.0) to automatically detect and filter stop\n        words based on intra corpus document frequency of terms.\n\n    lowercase : boolean, default True\n        Convert all characters to lowercase before tokenizing.\n\n    token_pattern : string\n        Regular expression denoting what constitutes a \"token\", only used\n        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n    max_df : float in range [0.0, 1.0] or int, default=1.0\n        When building the vocabulary ignore terms that have a document\n        frequency strictly higher than the given threshold (corpus-specific\n        stop words).\n        If float, the parameter represents a proportion of documents, integer\n        absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    min_df : float in range [0.0, 1.0] or int, default=1\n        When building the vocabulary ignore terms that have a document\n        frequency strictly lower than the given threshold. This value is also\n        called cut-off in the literature.\n        If float, the parameter represents a proportion of documents, integer\n        absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    max_features : int or None, default=None\n        If not None, build a vocabulary that only consider the top\n        max_features ordered by term frequency across the corpus.\n\n        This parameter is ignored if vocabulary is not None.\n\n    vocabulary : Mapping or iterable, optional\n        Either a Mapping (e.g., a dict) where keys are terms and values are\n        indices in the feature matrix, or an iterable over terms. If not\n        given, a vocabulary is determined from the input documents.\n\n    binary : boolean, default=False\n        If True, all non-zero term counts are set to 1. This does not mean\n        outputs will have only 0/1 values, only that the tf term in tf-idf\n        is binary. (Set idf and normalization to False to get 0/1 outputs.)\n\n    dtype : type, optional\n        Type of the matrix returned by fit_transform() or transform().\n\n    norm : 'l1', 'l2' or None, optional\n        Norm used to normalize term vectors. None for no normalization.\n\n    use_idf : boolean, default=True\n        Enable inverse-document-frequency reweighting.\n\n    smooth_idf : boolean, default=True\n        Smooth idf weights by adding one to document frequencies, as if an\n        extra document was seen containing every term in the collection\n        exactly once. Prevents zero divisions.\n\n    sublinear_tf : boolean, default=False\n        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n\n    Attributes\n    ----------\n    vocabulary_ : dict\n        A mapping of terms to feature indices.\n\n    idf_ : array, shape = [n_features], or None\n        The learned idf vector (global term weights)\n        when ``use_idf`` is set to True, None otherwise.\n\n    stop_words_ : set\n        Terms that were ignored because they either:\n\n          - occurred in too many documents (`max_df`)\n          - occurred in too few documents (`min_df`)\n          - were cut off by feature selection (`max_features`).\n\n        This is only available if no vocabulary was given.\n\n    See also\n    --------\n    CountVectorizer\n        Tokenize the documents and count the occurrences of token and return\n        them as a sparse matrix\n\n    TfidfTransformer\n        Apply Term Frequency Inverse Document Frequency normalization to a\n        sparse matrix of occurrence counts.\n\n    Notes\n    -----\n    The ``stop_words_`` attribute can get large and increase the model size\n    when pickling. This attribute is provided only for introspection and can\n    be safely removed using delattr or set to None before pickling.\n    ",
        "Hyperparameters": {
            "decode_error": "strict",
            "lowercase": true,
            "input": "content",
            "max_features": null,
            "stop_words": null,
            "encoding": "utf-8",
            "strip_accents": null,
            "dtype": null,
            "use_idf": true,
            "binary": false,
            "vocabulary": null,
            "smooth_idf": true,
            "token_pattern": "(?u)\\b\\w\\w+\\b",
            "norm": "l2",
            "analyzer": "word",
            "tokenizer": null,
            "sublinear_tf": false,
            "preprocessor": null,
            "min_df": 1,
            "max_df": 1.0,
            "ngram_range": [
                1,
                1
            ]
        }
    }
}